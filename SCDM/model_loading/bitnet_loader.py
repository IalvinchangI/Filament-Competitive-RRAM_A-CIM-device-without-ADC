import torch
import numpy
from transformers import AutoTokenizer, TextIteratorStreamer
from threading import Thread
import traceback
import gc

from model_loading import BasicModelLoader
from models import BitLinear
from utils import LoggingColor
from utils.environment_variables import HP_TOKEN, TORCH_DEVICE

from torch import nn
from typing import Union, Iterator
from SCDM_Driver import SCDM_DriverInterface


class TernaryBitNetLoader(BasicModelLoader):

    BIT_DEPTH = 8 # Á°¨È´îËº∏ÂÖ•‰ΩçÂÖÉÊ∑±Â∫¶ (x_quant ÊòØ int8)
    MAX_TOKEN = 50
    FIX_OUTPUT_GENERATION = False
    STREAM_GENERATE_TIMEOUT = None  # no timeout
    # STREAM_GENERATE_TIMEOUT = 20.0
    
    @classmethod
    def set_MAX_TOKEN(cls, value: int):
        if value <= 0:
            raise ValueError("MAX_TOKEN should greater than 0.")
        cls.MAX_TOKEN = value
    
    # Class Level ÁöÑ Tokenizer Âø´Âèñ
    _tokenizer = None
    _DEFAULT_TOKENIZER_ID = "microsoft/bitnet-b1.58-2B-4T"

    @classmethod
    def init_tokenizer(cls, id: str = None) -> bool:
        if cls._tokenizer is not None:
            return True
            
        try:
            cls._tokenizer = AutoTokenizer.from_pretrained(
                cls._DEFAULT_TOKENIZER_ID if (id is None) else id, 
                token=HP_TOKEN
            )
            if cls._tokenizer.pad_token is None:
                cls._tokenizer.pad_token = cls._tokenizer.eos_token
            return True
        except Exception as e:
            cls._tokenizer = None
            cls._logger.error(LoggingColor.color_text(f"‚ùå Failed to load tokenizer: {e}", LoggingColor.ERROR))
            return False

    GENERATE_COMMAND = "generate_command"
    INFERENCE_COMMAND = "inference_command"
    STREAM_GENERATE_COMMAND = "stream_generate_command"

    _logger = LoggingColor.get_logger("TernaryBitNetLoader")

    @classmethod
    def str2token(cls, string: str) -> numpy.ndarray:
        """Â∞áÂ≠ó‰∏≤ËΩâÊèõÁÇ∫ Token ID (Numpy Array)"""
        if cls._tokenizer is None:
            if cls.init_tokenizer() == False:
                return numpy.array([])

        encoded = cls._tokenizer(string, return_tensors="np")
        return encoded.input_ids

    @classmethod
    def token2str(cls, tokens: numpy.ndarray) -> str:
        """
        Â∞á Token ID ËΩâÊèõÁÇ∫Â≠ó‰∏≤
        ÊîØÊè¥Ëº∏ÂÖ•: ÂñÆ‰∏ÄÊï¥Êï∏, List, Numpy Array, Torch Tensor
        """
        if cls._tokenizer is None:
            if cls.init_tokenizer() == False:
                return ""

        # 1. ÂûãÂà•Ê®ôÊ∫ñÂåñ (Normalize to int or List[int])
        if isinstance(tokens, torch.Tensor):
            if tokens.numel() == 1:
                tokens = tokens.item()
            else:
                tokens = tokens.tolist()
        elif isinstance(tokens, numpy.ndarray):
            if tokens.size == 1:
                tokens = tokens.item()
            else:
                tokens = tokens.tolist()
        elif isinstance(tokens, (int, numpy.integer)):
            tokens = int(tokens)
            
        # 2. Ëß£Á¢º
        decoded = cls._tokenizer.decode(tokens, skip_special_tokens=True)
        return decoded

    @classmethod
    def print_stream(cls, stream_result: Iterator[str]) -> Iterator[str]:
        """
        Áõ¥Êé•Âç∞Âá∫ stream ÁöÑÊù±Ë•ø
        
        Usage:
            raw_stream = loader.run(..., cmd=STREAM_GENERATE_COMMAND)
            BitNetLoader.print_stream(raw_stream)
        """
        if stream_result is None:
            return

        try:
            for new_text in stream_result:
                print(new_text, end="", flush=True)
        except Exception as e:
            # TODO stop thread
            cls._logger.error(f"Stream decoding error: {e}")

    def __init__(self, model_path: str):
        super().__init__()
        # init model
        try:
            self._logger.info(f"‚è≥ Loading model from {model_path}")
            self.model: nn.Module = torch.load(model_path, map_location=TORCH_DEVICE, weights_only=False)
            self.model.eval()
            
            # Ëá™ÂãïÂÅµÊ∏¨ device
            self.device = next(self.model.parameters()).device
            
            self._logger.info(LoggingColor.color_text("‚úÖ Loaded.", LoggingColor.GREEN))
        except Exception as e:
            self._logger.error(LoggingColor.color_text(f"‚ùå Load failed: {e}", LoggingColor.ERROR))
            self.model = None

        # init driver
        self.driver: SCDM_DriverInterface = None

    def config(self, driver: SCDM_DriverInterface):
        """ÈÖçÁΩÆÁ°¨È´îÂä†ÈÄü"""
        super().config(driver)
        
        if self.model is None:
            return

        self._logger.info("‚öôÔ∏è Configuring BitNet hardware acceleration...")
        count = 0

        # submit weights
        for name, module in self.model.named_modules():
            if isinstance(module, BitLinear):
                # 1. Á¢∫‰øù Scale Â∑≤Ë®àÁÆó
                module.calculate_weight_scale()

                # 2. ÂèñÂæóÈáèÂåñÊ¨äÈáç
                w_quant_tensor = module.get_quantization_weights().to(torch.int8)
                w_quant_np = w_quant_tensor.cpu().numpy().T
                
                # 3. Setup Hardware
                gid = driver.submit_matrix([w_quant_np])
                
                # 4. Á∂ÅÂÆö ID
                module.layer_id = gid
                count += 1

                # 5. Âà™Èô§ PyTorch ÂéüÊú¨ÁöÑÊ¨äÈáçÂèÉÊï∏ÔºåÈáãÊîæË®òÊÜ∂È´î
                del module.weight
        
        # Âº∑Âà∂ÈÄ≤Ë°åÂûÉÂúæÂõûÊî∂‰∏¶Ê∏ÖÁ©∫ GPU Âø´Âèñ
        gc.collect()
        torch.cuda.empty_cache()
        
        self._logger.info(f"‚úÖ Configured {count} BitLinear layers to Hardware.")

        # Define Adapter
        def _matmul_hardware_adapter(x_quant_tensor, layer_id: str):
            x_np = x_quant_tensor.cpu().numpy().astype(numpy.int8)
            y_np = driver.compute_multibit(layer_id, x_np, self.BIT_DEPTH)
            y_tensor = torch.from_numpy(y_np).to(device=x_quant_tensor.device)
            return y_tensor

        # Global Switch
        BitLinear.set_matmul(_matmul_hardware_adapter)

    def run(self, input_data: numpy.ndarray, input_command: str = None) -> Union[numpy.ndarray, Iterator[str]]:
        """
        Âü∑Ë°åÊ®°Âûã
        Returns:
            numpy.ndarray: Ëã•ÁÇ∫ INFERENCE Êàñ GENERATE
            Iterator[str]: Ëã•ÁÇ∫ STREAM_GENERATE (ÂõûÂÇ≥ Python Generator)
        """
        super().run(input_data, input_command)

        if self.model is None:
            self._logger.error(LoggingColor.color_text("‚ùå Model not loaded.", LoggingColor.ERROR))
            return None
        
        # Prepare Input
        input_tensor = torch.from_numpy(input_data).to(self.device)
        if input_tensor.dim() == 1:
            input_tensor = input_tensor.unsqueeze(0)
        attention_mask = torch.ones_like(input_tensor)  # WARMING: The attention_mask is hard-coded as an all-ones array.

        self._logger.info(f"üöÄ Running {input_command}...")

        output_data = None

        try:
            if input_command == self.INFERENCE_COMMAND or input_command is None:
                # ÂñÆÊ¨°Êé®Ë´ñ
                with torch.no_grad():
                    outputs = self.model(input_tensor)
                    logits = outputs.logits
                output_data = logits.cpu().numpy()
            
            elif input_command == self.GENERATE_COMMAND:
                # ‰∏ÄËà¨ÁîüÊàê (Blocking)
                with torch.no_grad():
                    output_tensor = self.model.generate(
                        input_tensor, 
                        attention_mask=attention_mask, 
                        max_new_tokens=self.MAX_TOKEN, 
                        pad_token_id=self.model.config.eos_token_id, 
                        do_sample=(not self.FIX_OUTPUT_GENERATION)
                    )
                output_data = output_tensor.cpu().numpy()

            elif input_command == self.STREAM_GENERATE_COMMAND:
                # ‰∏≤ÊµÅÁîüÊàê (Non-Blocking / Generator)
                # 1. Á¢∫‰øù Tokenizer ËºâÂÖ• (Streamer ÈúÄË¶ÅÂÆÉ‰æÜËß£Á¢º)
                if self._tokenizer is None:
                    if not self.init_tokenizer():
                         return None
                
                # 2. Âª∫Á´ã Streamer (ÈÄôÊòØ‰∏ÄÂÄã Iterator)
                # skip_prompt=True: ‰∏çÈáçË§áÂõûÂÇ≥Ëº∏ÂÖ•ÁöÑÂ≠ó
                streamer = TextIteratorStreamer(self._tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=self.STREAM_GENERATE_TIMEOUT)

                # 3. Ë®≠ÂÆöÁîüÊàêÂèÉÊï∏
                generation_kwargs = dict(
                    inputs=input_tensor, 
                    attention_mask=attention_mask, 
                    streamer=streamer, 
                    max_new_tokens=self.MAX_TOKEN, 
                    pad_token_id=self.model.config.eos_token_id, 
                    do_sample=(not self.FIX_OUTPUT_GENERATION)
                )

                # 4. Âú®Â≠êÂü∑Ë°åÁ∑í‰∏≠ÂïüÂãïÁîüÊàê
                thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
                thread.start()

                # 5. Áõ¥Êé•ÂõûÂÇ≥ Streamer (Generator)
                # Â§ñÈÉ®ÂèØ‰ª•Áî® `for text in result: print(text)` ‰æÜÊé•Êî∂
                return TextStreamTracker(streamer)

            else:
                self._logger.warning(LoggingColor.color_text(f"‚ö†Ô∏è Unknown command: {input_command}", LoggingColor.WARNING))
                return None

        except Exception as e:
            self._logger.error(LoggingColor.color_text(f"‚ùå Runtime error: {e}", LoggingColor.ERROR))
            traceback.print_exc()
            return None

        return output_data
        
    def unload(self):
        super().unload()
        if self.model is None or self.driver is None:
            return

        self._logger.info("üßπ Unloading model resources...")
        count = 0
        for module in self.model.modules():
            if isinstance(module, BitLinear):
                if module.layer_id is not None:
                    if self.driver.clear_matrix(module.layer_id):
                        count += 1
                    module.layer_id = None 

        BitLinear.set_matmul(None) 
        self._logger.info(f"‚úÖ Cleared {count} matrices from hardware.")

    def loader_details(self):
        return {
            "bit_depth": self.BIT_DEPTH, 
            "fix_output_generation": self.FIX_OUTPUT_GENERATION, 
            "max_token": self.MAX_TOKEN
        }



class TextStreamTracker:
    """
    Áî®‰æÜÂåÖË£ù TextIteratorStreamer„ÄÇ
    Áï∂ÂâçÁ´Ø (main.py) Áî® for Ëø¥ÂúàËÆÄÂèñ‰∏¶Âç∞Âá∫Â≠ó‰∏≤ÊôÇÔºå
    ÂÆÉÊúÉÂÅ∑ÂÅ∑Âú®ËÉåÊôØÊääÂÆåÊï¥ÁöÑÂè•Â≠êÂ≠òËµ∑‰æÜÔºå‰æõÂæåÁ∫å Executor Â≠òÊ™î„ÄÇ
    """
    def __init__(self, streamer):
        self.streamer = streamer
        self._accumulated_text = ""
        self._on_finish_callback = None
    
    def register_on_finish_callback(self, callback):
        """ÂÖÅË®±Â§ñÈÉ®Ë®ªÂÜä‰∏ÄÂÄãÂú®‰∏≤ÊµÅÁµêÊùüÊôÇËß∏ÁôºÁöÑÂáΩÂºè"""
        self._on_finish_callback = callback
        
    def __iter__(self):
        try:
            for chunk in self.streamer:
                if isinstance(chunk, str):
                    self._accumulated_text += chunk
                yield chunk
        finally:
            if self._on_finish_callback is not None:
                self._on_finish_callback()
            
    def get_full_result(self) -> str:
        return self._accumulated_text
